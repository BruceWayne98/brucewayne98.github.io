---
title: "SILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores"
excerpt: "A comprehensive analysis of the SILK research paper from USENIX ATC 2019, which introduces an I/O scheduler to prevent tail latency spikes in LSM-based key-value stores through dynamic bandwidth allocation, prioritization, and preemptive compaction."
categories:
  - Database Systems
  - Storage Systems
tags:
  - Databases
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
author_profile: true
---

## Overview

**SILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores** is a seminal research paper published at USENIX ATC 2019 and awarded **Best Paper**. The research was conducted by Oana Balmau, Florin Dinu, and Willy Zwaenepoel from the University of Sydney, along with Karan Gupta and Ravishankar Chandhiramoorthi from Nutanix Inc., and Diego Didona from IBM Research.

SILK addresses a critical performance bottleneck in LSM-based key-value stores: **high tail latencies** caused by interference between client operations and internal maintenance tasks (flushes and compactions). The solution introduces an I/O scheduler that coordinates these operations to achieve up to **two orders of magnitude** improvement in 99th percentile latencies without compromising throughput.

**Paper Citation:**
```
Balmau, O., Dinu, F., Zwaenepoel, W., Gupta, K., Chandhiramoorthi, R., & Didona, D. (2019). 
SILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores. 
In 2019 USENIX Annual Technical Conference (USENIX ATC 19) (pp. 753-766).
```

---

## Background: LSM-Tree Architecture

### What is an LSM-Tree?

A **Log-Structured Merge Tree (LSM-tree)** is a write-optimized data structure used in modern key-value stores like RocksDB, LevelDB, Apache Cassandra, and many distributed databases. LSM-trees optimize write performance by buffering updates in memory and later organizing them into sorted files on disk.

### Core Components

LSM-based key-value stores consist of three main components:

#### 1. Memory Component (Cm)

- **Purpose**: Temporarily absorb incoming write operations
- **Structure**: Sorted in-memory data structure (e.g., skip list or B-tree)
- **Size**: Typically 64-256 MB
- **Behavior**: When full, it becomes immutable and is replaced by a new active memory component
- **Flush Operation**: The immutable component is asynchronously written to Level 0 (L0) on disk

#### 2. Disk Component (Cdisk)

The disk component is organized into multiple **levels** (L0, L1, ..., Ln), where each level is larger than the previous by a configurable factor (typically 10x).

**Structure:**
- **SSTables (Sorted String Tables)**: Immutable sorted files storing key-value pairs
- **L0 (Level 0)**: Contains recently flushed memtables; **allows overlapping key ranges** between SSTables
- **L1 to Ln**: Higher levels where SSTables have **disjoint key ranges** within each level
- **Size Growth**: Each level is approximately 10x larger than the previous level

**Data Layout Policies:**
- **Tiering**: Multiple sorted runs (groups of SSTables) per level
- **Leveling**: One sorted run per level (used in L1 and above in RocksDB)
- **Hybrid (1-Leveling)**: Tiering for L0, leveling for other levels (RocksDB default)

#### 3. Commit Log (Clog / Write-Ahead Log)

- **Purpose**: Durability and crash recovery
- **Operation**: Records updates before they're applied to the memory component
- **Size**: Typically a few hundred MB
- **Optional**: Can be disabled if durability is not required

### LSM Operations

#### Client Operations

**Write (Update):**
```
Update(key, value)
```
- Writes are first appended to the Write-Ahead Log (WAL)
- Then inserted into the active memtable (Cm)
- **Performance**: Very fast (in-memory operation)
- **Latency**: Typically microseconds

**Read (Get):**
```
Get(key) ‚Üí value
```
- Search order: Cm ‚Üí L0 ‚Üí L1 ‚Üí ... ‚Üí Ln
- **Optimization**: Bloom filters reduce unnecessary SSTable reads
- **Typical case**: Only one SSTable checked per level
- **Read Amplification**: Number of SSTables consulted per read

**Range Scan:**
```
Scan(key_start, key_end) ‚Üí [(k1,v1), (k2,v2), ...]
```
- Queries Cm and all relevant SSTables containing the key range
- Merges results from multiple levels
- Can be expensive with many overlapping files

#### Internal Operations

**Flushing:**
- **Trigger**: When memtable reaches size threshold
- **Operation**: Writes immutable memtable to L0 as an SSTable
- **Characteristic**: Written as-is without merge-sorting (hence L0 has overlapping key ranges)
- **Criticality**: Blocking flushes can stall writes when all memtables are full

**Compaction:**
- **Purpose**: 
  - Merge SSTables across levels
  - Remove obsolete/deleted data
  - Reduce read amplification
  - Maintain LSM tree structure
  
- **Process**:
  1. Select SSTable(s) from level Li
  2. Identify overlapping SSTables in Li+1
  3. Merge-sort selected SSTables
  4. Write new sorted SSTables to Li+1
  5. Delete old input SSTables

- **L0 ‚Üí L1 Compaction**: Special case
  - Only one L0‚ÜíL1 compaction can run at a time (due to overlapping ranges in L0)
  - **Critical for system health**: Prevents L0 from filling up
  
- **L1+ Compactions**: Can run in parallel
  - Multiple compactions can occur simultaneously at higher levels
  - More resource-intensive (larger data volumes)

### Amplification Factors

#### Write Amplification
**Definition**: Ratio of data written to storage vs. data written by application

For LSM-trees:
\[ \text{Write Amp} = 1 + 1 + \sum_{i=1}^{n} \text{merge factor}_i \]

- 1 for WAL write
- 1 for memtable flush
- Additional writes for each compaction level (typically ~7-10x per level)

**Total**: Typically 20-30x for a multi-level LSM tree

#### Read Amplification
**Definition**: Number of disk reads required to serve a single logical read

\[ \text{Read Amp} = O\left(\frac{(\log^2 N/B)}{\log k}\right) \]

Where:
- N = database size
- B = block/page size
- k = level growth factor

**Typical**: 5-10 SSTables checked per read (with Bloom filters)

#### Space Amplification
**Definition**: Ratio of storage used vs. actual live data size

Caused by:
- Multiple versions of same key across levels
- Tombstones (deletion markers)
- Uncompleted compactions

**Typical**: 1.1-1.5x of live data

---

## The Problem: Tail Latency Spikes

### Performance Requirements for LSM KV Stores

Modern LSM-based key-value stores must satisfy three critical requirements:

1. **Low Tail Latency**: Especially important for applications with high fan-out queries where the slowest response determines overall latency
2. **Predictable Throughput**: Ability to match client load without variability
3. **Small Memory Footprint**: Efficient memory usage when co-located with other services

### Root Causes of Latency Spikes

The SILK paper identifies **interference between client writes, flushes, and compactions** as the fundamental cause of tail latency spikes. This manifests in two primary scenarios:

#### Scenario 1: L0 Reaches Full Capacity

**Timeline of Events:**
```
t=0:  Multiple L1‚ÜíL2 compactions start
t=5:  Burst of client writes ‚Üí triggers multiple flushes
t=10: L0‚ÜíL1 compaction running but slow (limited bandwidth shared with L1‚ÜíL2 compactions)
t=15: L0 accumulates 7, 8, 9... SSTables
t=19: L0 reaches limit (10 SSTables)
      ‚ö†Ô∏è Flushes are temporarily PAUSED
      ‚ö†Ô∏è Memtable fills up
      ‚ö†Ô∏è Write operations BLOCKED
      üí• LATENCY SPIKE: 1-3 orders of magnitude increase
```

**Why This Happens:**
- Multiple parallel compactions share I/O bandwidth equally
- L0‚ÜíL1 compaction (non-parallelizable) gets only 1/N of bandwidth where N = total compactions
- L0‚ÜíL1 compaction slows down ‚Üí L0 not cleared fast enough
- L0 reaches maximum capacity ‚Üí flushes halted ‚Üí memtable fills ‚Üí writes blocked

#### Scenario 2: Slow Flushing Due to Compaction Interference

**Timeline of Events:**
```
t=0:  Flush starts
t=0-5: 7 concurrent L1‚ÜíL2 compactions running
      ‚Üí Flush shares bandwidth with compactions
      ‚Üí Flush takes 5 seconds instead of typical 1-2 seconds
t=5:  Another flush triggered, but previous flush still ongoing
      ‚ö†Ô∏è Active memtable fills up while waiting
      ‚ö†Ô∏è No space to absorb new writes
      üí• LATENCY SPIKE: Writes blocked
```

**Why This Happens:**
- Large number of parallel compactions monopolize I/O bandwidth
- Flush operations compete for limited bandwidth
- Slow flushes ‚Üí memtable remains full ‚Üí no buffer for incoming writes

### Why Existing Solutions Fail

#### Simple Rate Limiting (RocksDB with I/O Limits)

**Approach**: Cap I/O bandwidth for internal operations (e.g., 50 MB/s, 75 MB/s, 90 MB/s)

**Result**: 
- ‚úÖ Delays latency spikes temporarily
- ‚ùå Does not prevent them long-term
- ‚ùå Actually **worsens the problem** over time

**Why It Fails:**
- Rate limiting slows down compactions
- Compactions accumulate as backlog
- Eventually, many compactions need to run simultaneously
- When they do, bandwidth contention becomes even worse
- **Observation**: Systems with 50 MB/s limit showed spikes at ~900s, 90 MB/s at ~2500s

#### Increased Memory (Larger Memtables)

**Approach**: Allocate 1GB total memory (vs. typical 256MB)

**Configurations Tested:**
- 2 √ó 500MB memtables
- 10 √ó 100MB memtables

**Result**:
- ‚úÖ Delays spikes longer
- ‚ùå Eventually encounters same issues
- ‚ùå Violates memory footprint requirements for production systems

**Why It Fails:**
- Larger buffers only postpone the problem
- Root cause (interference) remains unaddressed

#### Reducing Compaction Overhead (TRIAD)

**Approach**: 
- Keep frequently updated keys in memory longer
- Use commit log data for flushing
- Trigger L0‚ÜíL1 compaction only when key-range overlap is significant

**Result**:
- ‚úÖ Good performance for first ~1,000 seconds
- ‚ùå Frequent and severe spikes after that point

**Why It Fails:**
- Postponing compactions leads to accumulation
- Eventually many compactions must run simultaneously
- Interference problem re-emerges more severely

#### Avoiding Compactions (PebblesDB - Fragmented LSM)

**Approach**: 
- Use "guards" (pointers) to organize SSTables
- Eliminate most compactions (only compact at highest level)
- Allow overlapping key ranges at all levels

**Result**:
- ‚úÖ Excellent tail latency initially
- ‚ùå High memory overhead (guards + Bloom filters)
- ‚ùå Runs out of memory in write-intensive workloads (~10,500s)
- ‚ùå When highest-level compaction eventually occurs, system stalls completely

**Why It Fails:**
- Memory consumption grows unbounded under write load
- Highest-level compaction is extremely expensive
- System halts during that compaction (all threads busy)

### Key Insights from Experimental Study

The SILK research identified three critical lessons:

**Lesson 1: Not All Internal Operations Are Equal**

Operations at lower levels (closer to memory) are **critical** because their delay directly blocks client operations:
- **Highest Priority**: Flushes (clear memtable to accept new writes)
- **Second Priority**: L0‚ÜíL1 compaction (prevent L0 from filling)
- **Third Priority**: Higher-level compactions (maintain tree structure)

**Lesson 2: Bandwidth Limiting Is Insufficient**

Simply capping internal operation bandwidth:
- Does not prevent interference during concurrent operations
- Can exacerbate problems by creating compaction backlog
- Needs intelligent coordination, not just throttling

**Lesson 3: Throughput Optimization ‚â† Latency Optimization**

Techniques that improve average throughput by reducing compaction cost:
- May reduce probability of spikes
- Do not eliminate spikes
- Can worsen tail latency in the long run by deferring necessary work

**Corollary**: Performance tests must run for **extended duration** (hours, not minutes) to detect these issues.

---

## SILK Solution: I/O Scheduler for LSM Trees

SILK introduces a novel **I/O scheduler** specifically designed for LSM-based key-value stores. The scheduler coordinates client operations with internal maintenance tasks through three core techniques.

### Design Principles

#### 1. Opportunistic Bandwidth Allocation

**Insight**: Production workloads have **variable load over time** (see Nutanix production trace showing fluctuations between 10-30 Kops/s).

**Strategy**:
- Dynamically monitor client I/O bandwidth usage
- Allocate **leftover bandwidth** to internal operations
- Reduce bandwidth for higher-level compactions during peak client load
- Increase bandwidth for internal operations during low-load periods

**Benefits**:
- Limits interference between client and internal operations
- Prevents long-term backlog accumulation
- Adapts to workload characteristics in real-time

#### 2. Prioritized Execution

**Categorization of Internal Operations** (by impact on client latency):

| Priority | Operation | Impact | Why Critical |
|----------|-----------|--------|--------------|
| **1 (Highest)** | Flush | Direct | Makes room in memtable; blocked flushes ‚Üí blocked writes |
| **2 (High)** | L0‚ÜíL1 Compaction | Direct | Prevents L0 from filling; full L0 ‚Üí blocked flushes |
| **3 (Normal)** | L1+ Compactions | Indirect | Maintains tree structure; less immediate impact |

**Implementation**:
- Dedicated high-priority thread pool for flushes
- Separate low-priority thread pool for compactions
- Guaranteed minimum bandwidth for critical operations
- Higher-level compactions can be paused/throttled

#### 3. Preemptive Compaction

**Problem**: L0‚ÜíL1 compaction may need to run while all compaction threads are busy with L1+ compactions.

**Solution**:
- Allow L0‚ÜíL1 compaction to **preempt** a running higher-level compaction
- Randomly select a higher-level compaction to pause
- Free up a thread for the critical L0‚ÜíL1 operation

**Handling Invalidated Work**:
- Preempted L0‚ÜíL1 compaction may invalidate partial work of higher-level compaction
- SILK discards invalidated work
- **Observation**: Wasted work has minimal performance impact (benefits outweigh costs)

### Implementation Details

#### Opportunistic Bandwidth Allocation Algorithm

**Monitoring and Adjustment**:

```python
# Pseudocode for SILK bandwidth allocation
MONITORING_GRANULARITY = 10  # milliseconds
ADJUSTMENT_THRESHOLD = 10    # MB/s
BUFFER_EPSILON = 5           # MB/s (safety margin)

def monitor_and_adjust_bandwidth():
    total_bandwidth = T  # Total available I/O bandwidth
    
    while True:
        # Measure client bandwidth over monitoring window
        client_bandwidth = measure_client_io()
        
        # Calculate available bandwidth for internal operations
        available_internal = total_bandwidth - client_bandwidth - BUFFER_EPSILON
        
        # Get current internal operation limit
        current_limit = get_current_rate_limit()
        
        # Only adjust if difference is significant (avoid thrashing)
        if abs(available_internal - current_limit) > ADJUSTMENT_THRESHOLD:
            set_internal_bandwidth_limit(available_internal)
        
        sleep(MONITORING_GRANULARITY)
```

**Key Parameters**:

- **Monitoring Granularity**: 10 ms (configurable based on workload fluctuation frequency)
- **Adjustment Threshold**: 10 MB/s (prevents excessive rate limit changes)
- **Buffer (Œµ)**: 5 MB/s (accounts for small fluctuations in client load)
- **Minimum Bandwidth**: Configured to ensure flush + L0‚ÜíL1 completion before next memtable fills

**Bandwidth Calculation**:
\[ I = T - C - \varepsilon \]

Where:
- I = Internal operation bandwidth
- T = Total system I/O bandwidth
- C = Client operation bandwidth (measured)
- Œµ = Buffer for fluctuations

#### Prioritization and Preemption Scheme

**Thread Pool Architecture**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Client Thread Pool (8 cores)     ‚îÇ
‚îÇ  Handles Get/Put/Scan operations    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   HIGH PRIORITY Thread Pool          ‚îÇ
‚îÇ   - Flush threads (1-2 threads)     ‚îÇ
‚îÇ   - L0‚ÜíL1 when promoted (0-1)       ‚îÇ
‚îÇ   - Guaranteed minimum bandwidth     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   LOW PRIORITY Thread Pool           ‚îÇ
‚îÇ   - L1+ compaction threads (4)      ‚îÇ
‚îÇ   - Dynamic bandwidth allocation     ‚îÇ
‚îÇ   - Can be paused/throttled          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Flush Handling**:
- Dedicated high-priority thread pool
- Always has access to internal operation bandwidth
- Minimum bandwidth: \( B_{min} \geq \frac{\text{Memtable Size}}{\text{Time to fill second memtable}} \)
- Current implementation: 1 flush thread, 2 memtables (1 active, 1 immutable)

**L0‚ÜíL1 Compaction Handling**:

```python
def handle_L0_to_L1_compaction():
    if L0_needs_compaction():
        if all_compaction_threads_busy_with_higher_levels():
            # Preemption needed
            preempted_compaction = randomly_select_running_L1plus_compaction()
            pause_compaction(preempted_compaction)
            
            # Temporarily promote L0‚ÜíL1 to high-priority pool
            run_L0_to_L1_in_high_priority_thread()
            
            # Share minimum bandwidth with flush
            # (At most 1 L0‚ÜíL1 compaction runs at a time)
        else:
            run_L0_to_L1_in_low_priority_thread()
    
    # Special case: L0‚ÜíL0 compaction (quickly reduce L0 SSTable count)
    # Treated same as L0‚ÜíL1
```

**Characteristics**:
- L0‚ÜíL1 compaction is **never paused** completely
- If no bandwidth allocated to compactions, L0‚ÜíL1 is temporarily moved to high-priority pool
- Shares minimum flush bandwidth when promoted
- At most 1 L0‚ÜíL1 compaction at a time (RocksDB constraint due to overlapping keys)
- Recent RocksDB supports L0‚ÜíL0 compaction (intra-L0 merging) - SILK treats this equally critical

**Higher-Level Compaction Handling**:

```python
def manage_higher_level_compactions():
    # Can be paused individually (preemption)
    # Can be paused as a group (high client load)
    # Use dynamic bandwidth from rate limiter
    
    if client_load_high():
        pause_all_L1plus_compactions()
    elif L0_to_L1_needs_thread():
        pause_random_L1plus_compaction()  # Preemption
    else:
        resume_paused_compactions()
        allocate_bandwidth_based_on_client_load()
```

**Parallel Compaction**:
- SILK supports parallel compactions (like RocksDB)
- Recommends **fewer threads** than typical guidance
- Traditional: # threads = # CPU cores
- SILK: # threads based on I/O bandwidth and compaction requirements
  - Example: 200 MB/s drive ‚Üí 4 compaction threads (50 MB/s each when all running)
  - Ensures each thread gets sufficient bandwidth to complete quickly

**Handling Invalidated Work**:
- When L0‚ÜíL1 preempts L1‚ÜíL2, the L1‚ÜíL2's partial work may be invalidated
- SILK discards the partial work
- Impact: Minimal (benefits of preventing L0 overflow outweigh wasted work)

#### Configuration and Tuning

**Key Configuration Parameters**:

| Parameter | Typical Value | Purpose |
|-----------|---------------|---------|
| Total I/O Bandwidth (T) | 200 MB/s | System-specific (measured) |
| Monitoring Granularity | 10 ms | Balance responsiveness vs. overhead |
| Adjustment Threshold | 10 MB/s | Prevent rate limit thrashing |
| Minimum Flush Bandwidth | 50 MB/s | Ensure timely memtable clearing |
| Compaction Threads | 4 | Based on I/O bandwidth |
| Memtable Count | 2 | Balance memory vs. burstiness |
| Memtable Size | 128 MB | System-specific |

**Auto-tuning Considerations**:
- SILK's dynamic bandwidth allocation provides inherent adaptability
- Minimum bandwidth thresholds should be set based on:
  - Write workload intensity
  - Memtable size
  - Hardware I/O capabilities

---

## Experimental Evaluation

### Experimental Setup

**Hardware Configuration**:
- **CPU**: 20-core Intel Xeon
- **Memory**: 256 GB RAM (restricted to 1GB using Linux cgroups for testing)
- **Storage**: 960 GB SSD
- **Total I/O Bandwidth**: ~200 MB/s

**Software**:
- **Base Systems**: RocksDB, TRIAD (modified RocksDB)
- **SILK Implementations**: RocksDB-SILK, TRIAD-SILK
- **Benchmark Tool**: `db_bench` (RocksDB's benchmarking utility)

**Workload Configuration**:
- **Dataset Size**: ~500 GB (pre-populated)
- **Key Size**: 16 bytes
- **Value Size**: 1 KB
- **Operations**: Mix of reads and writes (varies by workload)

**LSM Configuration**:
- **Memory Component**: 2 √ó 128 MB memtables
- **Minimum Bandwidth**: 50 MB/s (flush + L0‚ÜíL1)
- **Total I/O Cap**: 200 MB/s
- **Compaction Threads**: 4
- **Compression**: Disabled (for consistent comparison)
- **Commit Log**: Disabled (focus on LSM performance)
- **Thread Pinning**: 8 cores for client workers, 8 for internal operations

**Comparison Baselines**:
1. **RocksDB** (default configuration)
2. **RocksDB with Auto-tuned Rate Limiter** (adaptive I/O throttling)
3. **TRIAD** (state-of-the-art compaction optimization)

### Results: Nutanix Production Workload

**Workload Characteristics**:
- Real production trace from Nutanix deployment
- Write-dominated, bursty pattern
- Variable load: 10-30 Kops/s with peaks and valleys
- Duration: 2,000+ seconds, also tested for 24 hours

#### Tail Latency (99th Percentile)

**Results Summary**:

| System | P99 Latency (Peak) | Improvement Factor |
|--------|-------------------|-------------------|
| RocksDB | 10‚Å∂ - 10‚Å∑ Œºs (1-10 seconds) | Baseline |
| Auto-tuned RocksDB | 10‚Åµ - 10‚Å∂ Œºs (0.1-1 seconds) | 10x better |
| TRIAD | 10‚Åµ - 10‚Å∂ Œºs (0.1-1 seconds) | 10x better |
| **SILK** | **10¬≥ - 10‚Å¥ Œºs (1-10 ms)** | **100-1000x better** |

**Key Observations**:
- SILK achieves **up to 3 orders of magnitude** lower tail latency than RocksDB
- **2 orders of magnitude** improvement over auto-tuned RocksDB and TRIAD
- Latency remains **stable** throughout the experiment
- No latency spikes observed in SILK

#### Throughput Stability

**Observations**:

| System | Throughput Behavior |
|--------|---------------------|
| RocksDB | High fluctuations (10-50 Kops/s variance); frequent drops |
| Auto-tuned RocksDB | Moderate fluctuations; closer to client load |
| TRIAD | Similar to auto-tuned; still shows variability |
| **SILK** | **Tracks client load closely; minimal variance** |

**Quantitative Analysis**:
- SILK's throughput stays within 5% of client load
- RocksDB shows drops of 50%+ during compaction bursts
- SILK maintains predictable, stable throughput

#### Write Stalls

**Metric**: Frequency and duration of memtable stalls (writes blocked waiting for flush)

| System | Stall Count | Stall Duration (% of time) |
|--------|-------------|---------------------------|
| RocksDB | 150+ | 5-10% |
| Auto-tuned RocksDB | 20-30 | 1% |
| TRIAD | 69 | 2-3% |
| **SILK** | **0** | **0%** |

**Result**: SILK **never stalls** writes; flushes always complete before memtable fills.

#### Long-term Stability (24-hour Test)

**Test**: Production workload run continuously for 24 hours

**SILK Performance**:
- Compacted **~3 TB of data** over 24 hours
- Maintained **stable P99 latency** (~10 ms)
- **No backlog accumulation** observed
- System remained healthy with no degradation

**Comparison**:
- RocksDB: Frequent severe spikes throughout
- TRIAD: Increasing spike frequency over time
- Auto-tuned RocksDB: Eventual degradation after several hours

### Results: YCSB Synthetic Benchmarks

YCSB (Yahoo! Cloud Serving Benchmark) workloads tested with both uniform and Zipfian key distributions.

#### Workload Configurations

| Workload | Read % | Write % | Scan % | Focus |
|----------|--------|---------|--------|-------|
| A | 50 | 50 | 0 | Read-write mix |
| B | 95 | 5 | 0 | Read-heavy |
| C | 100 | 0 | 0 | Read-only |
| D | 95 | 0 | 5 | Read-latest |
| E | 0 | 0 | 100 | Scan-heavy |

#### Throughput Impact

**Write-Heavy Workloads (Workload A - 50/50)**:

| Key Distribution | SILK vs. TRIAD Throughput | Impact |
|------------------|---------------------------|--------|
| Uniform | -7% | Minimal overhead |
| Zipfian | -4% | Negligible |

**Read-Heavy Workloads (Workload B - 95/5)**:

| Key Distribution | SILK vs. TRIAD Throughput | Impact |
|------------------|---------------------------|--------|
| Uniform | -5% | Minimal |
| Zipfian | -2% | Negligible |

**Conclusion**: SILK's overhead on average throughput is **minimal** (at most 7%), with most workloads showing less than 5% difference.

#### Latency Improvements

**Median Latency**:
- SILK's median latency is **comparable or slightly better** than TRIAD across all workloads
- Negligible difference in typical-case performance

**Tail Latency (P99)**:

| Workload Type | Key Distribution | P99 Improvement |
|---------------|------------------|-----------------|
| Write-Heavy (A) | Uniform | **100-1000x** |
| Write-Heavy (A) | Zipfian | **50-100x** |
| Read-Heavy (B) | Uniform | **5-10x** |
| Read-Heavy (B) | Zipfian | **2-5x** |
| Read-Only (C) | Uniform | **5x** |
| Scan-Heavy (E) | Uniform | **10x** |

**Key Findings**:
- **Dramatic improvements** in write-heavy workloads (1-2 orders of magnitude)
- **Significant improvements** in read-heavy workloads (5-10x)
- **Consistent benefits** across different access patterns
- Zipfian distribution benefits from TRIAD's hot-key optimization, but SILK still shows major improvements

### Stress Testing: Variable Load Patterns

**Test Design**: Alternating high-load peaks and low-load valleys

**Configurations Tested**:

| Peak Duration | Valley Duration | Write % | Observation |
|---------------|-----------------|---------|-------------|
| 10 seconds | 10 seconds | 50% | SILK handles perfectly |
| 50 seconds | 10 seconds | 50% | SILK maintains stability |
| 100 seconds | 10 seconds | 50% | SILK degrades at ~700s |
| 10 seconds | 10 seconds | 90% | SILK degrades at ~500s |

#### Results

**Short Peaks (10-50 seconds)**:

| System | P99 Latency | Throughput Stability |
|--------|-------------|---------------------|
| RocksDB | 10‚Å∂ Œºs (spikes) | Highly variable |
| **SILK** | **10¬≥-10‚Å¥ Œºs** | **Tracks load closely** |

- SILK successfully **absorbs peaks** using low-load valleys for compaction catch-up
- Bandwidth allocation adapts in real-time to load changes

**Medium Peaks (100 seconds, 50% write)**:
- SILK maintains good performance up to **~700 seconds**
- Eventually, backlog accumulates as compaction cannot keep pace
- Still significantly outperforms RocksDB and TRIAD

**Intense Peaks (90% write)**:
- More challenging workload
- SILK degrades around **~500 seconds** of sustained peak
- Root cause: Write intensity exceeds system's compaction capacity even with optimization

**Conclusion**: SILK handles **realistic variable workloads** very well. Only extreme, sustained high-write scenarios eventually overwhelm the system.

### Ablation Study: Impact of Individual Techniques

**Question**: How much does each SILK technique contribute to overall improvement?

**Configurations Tested**:

1. **Dynamic Bandwidth Allocation Only**: Opportunistic I/O allocation without prioritization/preemption
2. **Prioritization + Preemption Only**: Priority-based scheduling without dynamic bandwidth
3. **Full SILK**: All techniques combined

#### Results

**Dynamic Bandwidth Only**:
- ‚úÖ Reduces interference between client and internal operations
- ‚ö†Ô∏è Urgent operations (flush, L0‚ÜíL1) slow down as experiment progresses
- ‚ö†Ô∏è Cannot prevent L0 overflow during sustained peaks
- **Conclusion**: Necessary but **insufficient** alone

**Prioritization + Preemption Only**:
- ‚úÖ Maintains LSM tree structure near memory (L0, L1 healthy)
- ‚ö†Ô∏è Uncontrolled bandwidth for higher-level compactions causes negative interference
- ‚ö†Ô∏è Large L1+ compactions can still slow down critical operations
- **Conclusion**: Critical for short-term stability, but needs bandwidth control for long-term health

**Full SILK (Both Techniques)**:
- ‚úÖ Maintains low tail latency **throughout** experiment
- ‚úÖ Stable throughput matching client load
- ‚úÖ No write stalls or L0 overflows
- **Conclusion**: Techniques are **complementary**; both required for optimal performance

**Key Insight**: The two mechanisms address different aspects of the problem:
- **Dynamic allocation**: Prevents long-term backlog and adapts to load
- **Prioritization/Preemption**: Ensures critical operations never blocked short-term

---

## Key Contributions

### 1. Empirical Study of Tail Latency in LSM KV Stores

**Contribution**: First comprehensive documentation that tail latency (not just average latency or throughput) is a **critical unsolved problem** in LSM-based systems.

**Findings**:
- Demonstrated 2-4 orders of magnitude difference between P50 and P99 latencies
- Showed that state-of-the-art throughput optimizations (TRIAD, PebblesDB) do not address tail latency
- Identified root causes through detailed profiling

**Impact**: Shifted research focus toward tail latency as a first-class optimization target.

### 2. I/O Scheduler Concept for LSM Trees

**Contribution**: Introduced the **first I/O scheduler** specifically designed for coordinating LSM internal and external operations.

**Novel Aspects**:
- Treats LSM operations with **different priorities** based on their criticality
- Dynamically adapts to workload characteristics
- Coordinates bandwidth allocation across operation types

**Contrast with Previous Work**:
- Prior: Optimize individual operations (e.g., faster compaction algorithms)
- SILK: Coordinate **all** operations holistically

### 3. Three Core Scheduling Techniques

**Techniques**:
1. **Opportunistic Bandwidth Allocation**: Dynamic, workload-aware I/O distribution
2. **Operation Prioritization**: Critical operations guaranteed resources
3. **Preemptive Compaction**: Lower-level operations can interrupt higher-level work

**Novelty**: First system to combine these techniques in a unified scheduler for LSM trees.

### 4. Production-Ready Implementation

**Contribution**: Open-source implementation in **RocksDB** and **TRIAD**

**Significance**:
- Industry-standard system (RocksDB used by Facebook, LinkedIn, Netflix, etc.)
- Demonstrates practical applicability
- Enables adoption in real-world systems

**Availability**: [GitHub - SILK-USENIXATC2019](https://github.com/theoanab/SILK-USENIXATC2019)

### 5. Extensive Experimental Validation

**Contribution**: Rigorous evaluation using:
- Real production workload from Nutanix
- Industry-standard benchmarks (YCSB)
- Long-duration tests (24 hours)
- Ablation studies

**Key Results**:
- **100-1000x** tail latency improvement in write-heavy workloads
- **Zero** write stalls
- **Minimal** (<7%) throughput overhead
- **Long-term stability** demonstrated over 24 hours and 3TB of compaction

---

## Limitations and Future Work

### Known Limitations

#### 1. Eventual Degradation Under Extreme Load

**Limitation**: SILK degrades when write intensity exceeds system's maximum compaction capacity for sustained periods.

**Example Scenarios**:
- 90% write workload with continuous high load (degrades at ~500s)
- 50% write with very long peaks (degrades at ~700s)

**Root Cause**: Fundamental I/O capacity limit; no scheduler can overcome hardware bandwidth constraints.

**Mitigation Strategies**:
- Scale horizontally (distribute writes across multiple nodes)
- Upgrade storage hardware (faster SSDs, NVMe)
- Implement admission control to limit incoming write rate

#### 2. Single-Node Focus

**Limitation**: SILK is designed and evaluated for single-node LSM stores.

**Unaddressed Scenarios**:
- Distributed LSM systems (e.g., distributed Cassandra, distributed RocksDB)
- Cross-node compaction coordination
- Network bandwidth as additional constraint

**Future Work**: Extend scheduler to distributed settings.

#### 3. Hardware-Specific Tuning

**Limitation**: Some parameters (e.g., minimum bandwidth thresholds, thread counts) require hardware-specific tuning.

**Challenge**: Optimal configuration may vary based on:
- SSD vs. HDD performance characteristics
- NVMe vs. SATA bandwidth
- CPU core count and power
- Memory capacity

**Future Work**: Auto-tuning mechanisms for parameter selection.

### Potential Extensions

#### 1. Finer-Grained Bandwidth Allocation

**Current**: Dynamic allocation at high/low priority pool level

**Extension**: Per-compaction bandwidth based on urgency:
- L1‚ÜíL2 compactions when L1 is near capacity: Higher priority
- L4‚ÜíL5 compactions when L4 is only 30% full: Lower priority
- Gradient of priorities rather than binary high/low

**Benefit**: Even more responsive to LSM tree state.

#### 2. Workload-Aware Scheduling

**Current**: General scheduler for all workloads

**Extension**: Adapt scheduling policy based on detected workload patterns:
- Read-heavy workload: Prioritize compactions more (improve read performance)
- Write-heavy workload: Current SILK policy (prioritize flushes)
- Scan-heavy workload: Balance between reducing SSTables and maintaining write capacity

**Approach**: Machine learning-based workload classification and policy selection.

#### 3. Integration with Admission Control

**Idea**: Combine SILK's scheduling with client request admission control.

**Mechanism**:
- Monitor LSM health (L0 SSTable count, pending compactions, etc.)
- Temporarily slow down incoming writes when system approaches overload
- Prevent complete system saturation

**Benefit**: Graceful degradation instead of sudden latency spikes.

#### 4. Hardware-Accelerated Compaction

**Context**: Emerging hardware acceleration (e.g., eBPF for sorting, FPGA for merge operations).

**Integration with SILK**:
- Offload compaction computation to accelerators
- Reduce CPU overhead of compactions
- Potentially increase compaction throughput within same I/O bandwidth

**Benefit**: Higher sustainable write rate.

#### 5. Tiered Storage Support

**Context**: Modern systems use heterogeneous storage (NVMe + SSD + HDD).

**Extension**: SILK scheduler aware of storage tier:
- Hot data (L0, L1) on NVMe
- Warm data (L2, L3) on SSD
- Cold data (L4+) on HDD
- Bandwidth allocation aware of per-tier characteristics

**Benefit**: Optimize cost-performance tradeoff.

### Research Directions

#### 1. Formal Model of LSM Scheduling

**Goal**: Develop theoretical framework for optimal LSM I/O scheduling.

**Questions**:
- What is the provably optimal scheduling policy under given workload?
- What are theoretical bounds on achievable tail latency?
- How to formally model trade-offs between latency, throughput, and resource utilization?

#### 2. Universal Compaction Scheduler

**Goal**: Generalize SILK to work across different LSM implementations (Cassandra, LevelDB, ScyllaDB, etc.).

**Challenge**: Each system has different compaction strategies (tiering vs. leveling, partial vs. full, etc.).

**Approach**: Abstract scheduler interface independent of underlying compaction algorithm.

#### 3. Adaptive Learning-Based Scheduler

**Goal**: Scheduler that learns optimal policies from workload history.

**Mechanism**:
- Online learning of workload patterns
- Reinforcement learning for scheduling decisions
- Continuous adaptation to evolving workloads

**Potential**: Outperform hand-tuned policies in complex, dynamic environments.

---

## Practical Implications

### For Database Developers

**Key Takeaways**:

1. **Tail Latency Matters**: Optimizing average latency or throughput is insufficient for production systems.

2. **Coordinate, Don't Just Optimize**: Coordinating operations (via scheduling) can be more effective than optimizing individual operations in isolation.

3. **Prioritization is Critical**: Not all internal operations have equal impact on client-facing performance.

4. **Dynamic Adaptation Works**: Workload-aware dynamic resource allocation significantly outperforms static configuration.

**Implementation Recommendations**:

- Implement separate thread pools for critical vs. non-critical operations
- Monitor client load continuously and adjust internal operation bandwidth
- Ensure flushes always have enough bandwidth to complete before memtables fill
- Use preemption to handle urgent operations even when system is busy
- Test under long-duration, variable workloads (not just steady-state benchmarks)

### For System Operators

**Deployment Guidance**:

1. **Upgrade to SILK-Enabled Systems**: If using RocksDB, consider SILK fork for latency-sensitive applications.

2. **Configuration**:
   - Set compaction thread count based on I/O bandwidth, not CPU cores
   - Configure minimum bandwidth for flush + L0‚ÜíL1 based on write rate
   - Enable monitoring of P95, P99, P99.9 latencies (not just average)

3. **Capacity Planning**:
   - Plan I/O capacity for **peak write load**, not average
   - Ensure sufficient headroom for compaction during peaks
   - Monitor L0 SSTable count as early warning signal

4. **Performance Monitoring**:
   - Track tail latencies continuously
   - Monitor write stall events
   - Alert on L0 SSTable count approaching limits

### For Application Developers

**Design Considerations**:

1. **Fan-out Queries**: SILK particularly benefits applications with high fan-out (slowest response determines latency).

2. **Write-Heavy Applications**: Greatest benefit for applications with:
   - High write rates
   - Bursty write patterns
   - Need for predictable latency

3. **Trade-offs**:
   - Minimal throughput overhead (<7% in most cases)
   - Significantly improved tail latency (often 100-1000x)
   - No additional memory requirements

**Use Cases**:
- Real-time analytics
- Time-series databases
- Metadata stores for distributed file systems
- Session stores for web applications
- IoT data ingestion

---

## Comparison with Related Work

### LSM Optimization Landscape

| Approach | Examples | Focus | Latency Impact | SILK Relationship |
|----------|----------|-------|----------------|-------------------|
| **Reduce Compaction Cost** | TRIAD, dCompaction | Throughput | Indirect | Complementary |
| **Fragmented LSM** | PebblesDB | Eliminate compactions | Short-term gain | Incompatible |
| **Tiered Storage** | Dotori, LSbM-tree | Storage efficiency | Indirect | Complementary |
| **Learned Indexes** | Bourbon, LISA | Query optimization | Orthogonal | Compatible |
| **Workload-Aware** | Dostoevsky, DOPA-DB | Adaptive compaction | Moderate | Complementary |
| **I/O Scheduling** | **SILK** | Tail latency | **Direct** | **- - -** |

### SILK vs. Rate Limiting

**Traditional Rate Limiting (RocksDB Auto-tuned)**:
- **Approach**: Cap total bandwidth for internal operations
- **Adaptation**: Multiplicative increase/decrease based on pending work
- **Granularity**: Coarse (all internal operations treated equally)
- **Result**: Delays problem, doesn't solve it

**SILK**:
- **Approach**: Dynamic allocation based on client load + prioritization
- **Adaptation**: Real-time monitoring (10ms granularity) + preemption
- **Granularity**: Fine (flush vs. L0‚ÜíL1 vs. L1+ compactions)
- **Result**: Prevents problem proactively

**Improvement**: 100x better tail latency over auto-tuned rate limiting.

### SILK vs. TRIAD

**TRIAD**:
- **Focus**: Reduce compaction frequency and cost
  - Keep hot keys in memory longer
  - Leverage commit log for flushing
  - Trigger L0‚ÜíL1 only when overlap is significant
- **Benefit**: Improved throughput
- **Limitation**: Defers compactions ‚Üí eventual burst of concurrent compactions ‚Üí latency spikes

**SILK**:
- **Focus**: Coordinate operations regardless of compaction cost
- **Benefit**: Stable tail latency even when compactions occur
- **Approach**: Complementary to TRIAD (SILK can be built on top of TRIAD)

**SILK+TRIAD**: Combined system (TRIAD-SILK) outperforms both individually.

### SILK vs. PebblesDB

**PebblesDB (Fragmented LSM)**:
- **Approach**: Avoid compactions by using guards (metadata pointers)
- **Benefit**: Extremely low write amplification
- **Limitation**: 
  - High memory overhead
  - Eventual highest-level compaction is catastrophic
  - Write-intensive workloads run out of memory

**SILK**:
- **Approach**: Make compactions coexist with client operations
- **Benefit**: Sustainable under continuous write load
- **Memory**: Standard LSM memory footprint

**Trade-off**: PebblesDB better for read-heavy; SILK better for write-heavy and mixed workloads.

### SILK+ (Follow-up Work)

**Extension**: SILK+ published in ACM TOCS 2020

**New Contributions**:
- Handles **heterogeneous workloads** (read-heavy and write-heavy mixed)
- Additional optimizations for read-intensive periods
- More sophisticated bandwidth allocation for mixed operation types

**Relationship**: SILK+ builds on original SILK with extended scope.

---

## Conclusion

### Summary

The **SILK** paper addresses a critical but previously under-studied problem in LSM-based key-value stores: **tail latency spikes** caused by interference between client operations and internal maintenance tasks.

**Core Innovation**: Introduction of an **I/O scheduler** specifically designed for LSM trees that:
1. Dynamically allocates bandwidth between client and internal operations
2. Prioritizes critical operations (flushes and L0‚ÜíL1 compactions)
3. Allows preemption of less critical work

**Impact**:
- **100-1000x** improvement in tail latency for write-heavy workloads
- **Minimal** throughput overhead (<7%)
- **Zero** write stalls
- **Proven** long-term stability (24 hours, 3TB compaction)

### Significance

**Academic Impact**:
- **Best Paper Award** at USENIX ATC 2019
- **213+ citations** (as of 2024)
- Inspired follow-up research on LSM scheduling, tail latency, and workload-aware systems

**Industry Impact**:
- Open-source implementation enables adoption in production systems
- Influenced RocksDB development roadmap
- Deployed at Nutanix and other companies

**Paradigm Shift**:
- From "optimize individual operations" to "coordinate all operations"
- From "throughput-first" to "tail latency as first-class metric"
- From "static configuration" to "dynamic, workload-aware adaptation"

### Lessons Learned

1. **Holistic Approach**: System-level coordination often more effective than component-level optimization.

2. **Tail Latency is Different**: Techniques that improve average performance may worsen tail latency.

3. **Workload Variability**: Real workloads fluctuate; systems must adapt dynamically.

4. **Prioritization Matters**: Not all operations are equal; critical-path operations need guaranteed resources.

5. **Long-term Testing Essential**: Performance problems may not manifest in short benchmark runs.

### Relevance to Modern Systems

**Current Trends**:
- **Disaggregated Storage**: Compute-storage separation in cloud (SILK principles apply to remote compaction)
- **Multi-tenancy**: Multiple LSM instances on shared resources (SILK scheduling per-tenant)
- **NVMe/Persistent Memory**: Faster storage changes I/O characteristics (SILK parameters need adjustment)
- **Machine Learning Workloads**: High write rates with unpredictable patterns (SILK's dynamic adaptation crucial)

**Future Evolution**:
- Integration with learned systems (ML-based workload prediction)
- Extension to distributed LSM (cross-node coordination)
- Hardware acceleration (offload compaction, focus SILK on scheduling)

### Final Thoughts

SILK demonstrates that **fundamental problems in widely-used systems can still be addressed with novel approaches**. By reconceptualizing LSM internal operations as a scheduling problem rather than just an implementation detail, the paper achieved dramatic improvements in a critical metric (tail latency) without sacrificing other performance aspects.

For researchers, SILK provides a template for **workload-aware, adaptive system design**. For practitioners, it offers **practical techniques** that can be immediately applied to production systems. The paper's impact extends beyond LSM trees, influencing how we think about resource management, priority scheduling, and tail latency optimization in storage systems broadly.

**Key Message**: In the era of cloud computing and microservices, where applications depend on fan-out queries and tail latency determines user experience, systems like SILK that prioritize predictable, low tail latency are not just desirable‚Äîthey are **essential**.

---

## References

**Primary Paper**:
- Balmau, O., Dinu, F., Zwaenepoel, W., Gupta, K., Chandhiramoorthi, R., & Didona, D. (2019). SILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores. In *2019 USENIX Annual Technical Conference (USENIX ATC 19)* (pp. 753-766). [PDF](https://www.usenix.org/system/files/atc19-balmau.pdf)

**Follow-up Work**:
- Balmau, O., Dinu, F., Zwaenepoel, W., Gupta, K., Chandhiramoorthi, R., & Didona, D. (2020). SILK+: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores Running Heterogeneous Workloads. *ACM Transactions on Computer Systems (TOCS)*, 36(4), 1-27.

**Related Systems**:
- RocksDB: [https://rocksdb.org/](https://rocksdb.org/)
- LevelDB: [https://github.com/google/leveldb](https://github.com/google/leveldb)
- Apache Cassandra: [https://cassandra.apache.org/](https://cassandra.apache.org/)

**SILK Source Code**:
- GitHub: [https://github.com/theoanab/SILK-USENIXATC2019](https://github.com/theoanab/SILK-USENIXATC2019)

**Foundational LSM Work**:
- O'Neil, P., Cheng, E., Gawlick, D., & Onuegbe, E. (1996). The log-structured merge-tree (LSM-tree). *Acta Informatica*, 33(4), 351-385.

**Related Compaction Optimization**:
- Balmau, O., Guerraoui, R., Trigonakis, V., & Zablotchi, I. (2017). FloDB: Unlocking memory in persistent key-value stores. In *Proceedings of EuroSys 2017*.
- Raju, P., Kadekodi, R., Chidambaram, V., & Abraham, I. (2017). PebblesDB: Building key-value stores using fragmented log-structured merge trees. In *Proceedings of SOSP 2017*.
- Dayan, N., & Idreos, S. (2018). Dostoevsky: Better space-time trade-offs for LSM-tree based key-value stores via adaptive removal of superfluous merging. In *Proceedings of SIGMOD 2018*.

**Tail Latency Research**:
- Dean, J., & Barroso, L. A. (2013). The tail at scale. *Communications of the ACM*, 56(2), 74-80.

---

## Appendix: Glossary of Terms

**Bloom Filter**: Probabilistic data structure that quickly tests whether an element is a member of a set. Used in LSM trees to avoid reading SSTables that definitely don't contain a key.

**Compaction**: Process of merging multiple SSTables from one or more levels, removing obsolete data, and writing results to a higher level.

**Flush**: Operation that writes an immutable memtable to disk as an SSTable in L0.

**Latency Percentiles**:
- **P50 (Median)**: 50% of operations complete faster than this value
- **P99 (99th percentile)**: 99% of operations complete faster than this value; critical for tail latency
- **P99.9**: 99.9% of operations complete faster than this value

**Leveling**: LSM data layout where each level contains exactly one sorted run (RocksDB default for L1+).

**LSM-tree**: Log-Structured Merge Tree; write-optimized data structure that buffers writes in memory and organizes disk data into sorted levels.

**Memtable**: In-memory buffer (sorted data structure) that absorbs incoming writes before being flushed to disk.

**Read Amplification**: Number of disk reads required to serve a single logical read operation.

**Space Amplification**: Ratio of physical storage used to actual live data size.

**SSTable**: Sorted String Table; immutable file on disk containing sorted key-value pairs.

**Tail Latency**: Latency at high percentiles (P95, P99, P99.9); represents worst-case user experience.

**Tiering**: LSM data layout where each level can contain multiple sorted runs.

**WAL (Write-Ahead Log)**: Append-only log that records operations before they're applied, ensuring durability.

**Write Amplification**: Ratio of data written to storage vs. data written by the application.

**Write Stall**: Situation where client writes are blocked because the memtable is full and cannot flush (often due to L0 being full).

---

*This blog post provides a comprehensive technical overview of the SILK paper for educational purposes. For production deployment, please refer to the original paper and source code repository.*